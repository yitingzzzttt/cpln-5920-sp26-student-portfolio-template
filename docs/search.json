[
  {
    "objectID": "Labs/Lab_2/lab2_template.html",
    "href": "Labs/Lab_2/lab2_template.html",
    "title": "Lab 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Labs/Lab_2/lab2_template.html#assignment-overview",
    "href": "Labs/Lab_2/lab2_template.html#assignment-overview",
    "title": "Lab 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Labs/Lab_2/lab2_template.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "Labs/Lab_2/lab2_template.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Lab 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\nYour Task:\nQuestions to answer:\n\nHow many hospitals are in your dataset?\n223\nHow many census tracts?\n3445\nWhat coordinate reference system is each dataset in?\npa_hospitals=WGS 84 ,“EPSG”,4326 pa_tracts=NAD83,“EPSG”,4269 pa_counties=WGS 84,“EPSG”,3857\n\n\n\nStep 2: Get Demographic Data\nYour Task:\n\n# Get demographic data from ACS\nlibrary(tidycensus)\nvars &lt;- c(\n  total_pop = \"B01001_001\",     \n  med_income = \"B19013_001\",    \n  m65_66 = \"B01001_020\", m67_69 = \"B01001_021\", m70_74 = \"B01001_022\", \n  m75_79 = \"B01001_023\", m80_84 = \"B01001_024\", m85_plus = \"B01001_025\",\n  f65_66 = \"B01001_044\", f67_69 = \"B01001_045\", f70_74 = \"B01001_046\", \n  f75_79 = \"B01001_047\", f80_84 = \"B01001_048\", f85_plus = \"B01001_049\"\n)\npa_census_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = vars,\n  state = \"PA\",\n  year = 2022, \n  output = \"wide\" \n)\npa_demographics &lt;- pa_census_data %&gt;%\n  mutate(\n    elderly_pop = rowSums(select(., m65_66E:f85_plusE)),\n    elderly_pct = (elderly_pop / total_popE) * 100\n  ) %&gt;%\n  rename(\n    total_population = total_popE,\n    median_income = med_incomeE\n  ) %&gt;%\n  select(GEOID, NAME, total_population, median_income, elderly_pop, elderly_pct)\n\n# Join to tract boundaries\npa_tracts_with_data &lt;- pa_tracts %&gt;%\n  left_join(pa_demographics, by = \"GEOID\")\n\nQuestions to answer:\n\nWhat year of ACS data are you using?\n2022\nHow many tracts have missing income data?\n62\nWhat is the median income across all PA census tracts?\n70188\n\n\n\n\nStep 3: Define Vulnerable Populations\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\npa_valid_data &lt;- pa_tracts_with_data %&gt;%\n  filter(!is.na(median_income)) %&gt;%\n  filter(total_population &gt; 0)\n\nincome_limit &lt;- 55924\nelderly_limit&lt;- 37.7\n\n\npa_vulnerable_populations &lt;- pa_valid_data %&gt;%\n  mutate(\n    is_vulnerable = ifelse(\n      !is.na(median_income) & \n      median_income &lt;= income_limit & \n      elderly_pct &gt;= elderly_limit, \n      TRUE, \n      FALSE\n    )\n  ) %&gt;%\n  filter(is_vulnerable == TRUE)\n\nQuestions to answer:\n\nWhat income threshold did you choose and why?\n55924, I use the 75% quantile. quantile(pa_valid_data$elderly_pct,0.25, na.rm = TRUE)\nWhat elderly population threshold did you choose and why?\n37.7, I found that the 75th percentile is approximately 25%. This means this threshold helps me identify the top 25% of communities with the highest aging levels across the state. quantile(pa_valid_data$elderly_pct,0.75, na.rm = TRUE)\nHow many tracts meet your vulnerability criteria?\n209\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n209/3445=6.07%\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\npa_vulnerable_projected &lt;- st_transform(pa_vulnerable_populations, 2272)\npa_hospitals_projected &lt;- st_transform(pa_hospitals, 2272)\n\nvulnerable_centroids &lt;- st_centroid(pa_vulnerable_projected)\n\ndistance&lt;- st_distance(vulnerable_centroids,pa_hospitals_projected)\n\npa_vulnerable_projected$dist_to_hospital_miles &lt;- as.numeric(apply(distance, 1, min)) / 5280\n\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n3.538635\nWhat is the maximum distance?\n19.15822\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n7\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\npa_vulnerable_projected&lt;- pa_vulnerable_projected %&gt;%\n  mutate(is_underserved = dist_to_hospital_miles &gt; 15)\n\nQuestions to answer:\n\nHow many tracts are underserved? 7\nWhat percentage of vulnerable tracts are underserved? 7/209=3.34%\nDoes this surprise you? Why or why not? Yes, because there are almost 20,000 of population have to go to hospital that is more than 15 miles away. —\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\npa_counties_projected &lt;- st_transform(pa_counties, 2272)\nvulnerable_with_county &lt;- st_join(pa_vulnerable_projected, pa_counties_projected)\n\n# Aggregate statistics by county\ncounty_summary &lt;- vulnerable_with_county %&gt;%\n  group_by(NAMELSADCO) %&gt;%\n  summarize(\n    num_vulnerable_tracts=n(),\n    num_underserved_tracts = sum(is_underserved),\n    avg_distance=mean(dist_to_hospital_miles),\n    total_vulnerable_population=sum(total_population,na.rm = TRUE))%&gt;%\n      mutate(\n        pct_vul_tracts_underserved=((num_underserved_tracts / num_vulnerable_tracts)*100)\n      )\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\nForest County, Juniata County, Monroe County, Sullivan County,Dauphin County\nWhich counties have the most vulnerable people living far from hospitals?\nForest County\nAre there any patterns in where underserved counties are located?\nUnderserved counties are primarily concentrated in the rural, mountainous central and northern regions of PA, where low population density leads to sparse hospital coverage.\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\nlibrary(knitr)\n# Create and format priority counties table\ntop_10_counties&lt;-county_summary%&gt;%\n  st_drop_geometry()%&gt;%\n  arrange(desc(pct_vul_tracts_underserved))%&gt;%\n  head(10)%&gt;%\n  mutate(\n    avg_distance = round(avg_distance, 1)\n  )\n  \n\ntop_10_counties %&gt;%\n  kable(\n    col.names = c(\"County Name\", \"Vulnerable Tracts\", \"Underserved Tracts\", \n                  \"Avg. Distance (mi)\", \"Total Vulnerable Pop\", \"% Underserved\"),\n    caption = \"Top 10 Pennsylvania Counties Prioritized for Healthcare Investment\",\n    align = \"lccccc\"\n  )\n\n\nTop 10 Pennsylvania Counties Prioritized for Healthcare Investment\n\n\n\n\n\n\n\n\n\n\nCounty Name\nVulnerable Tracts\nUnderserved Tracts\nAvg. Distance (mi)\nTotal Vulnerable Pop\n% Underserved\n\n\n\n\nForest County\n4\n4\n18.1\n10804\n100\n\n\nJuniata County\n5\n5\n15.9\n8910\n100\n\n\nMonroe County\n1\n1\n17.7\n1299\n100\n\n\nSullivan County\n4\n4\n16.9\n3672\n100\n\n\nDauphin County\n4\n3\n14.6\n16446\n75\n\n\nBradford County\n2\n1\n9.3\n8736\n50\n\n\nClearfield County\n4\n2\n9.0\n17256\n50\n\n\nAllegheny County\n35\n0\n2.3\n78986\n0\n\n\nArmstrong County\n1\n0\n3.1\n2850\n0\n\n\nBeaver County\n7\n0\n3.1\n13889\n0"
  },
  {
    "objectID": "Labs/Lab_2/lab2_template.html#part-2-comprehensive-visualization",
    "href": "Labs/Lab_2/lab2_template.html#part-2-comprehensive-visualization",
    "title": "Lab 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\ncounty_summary_clean &lt;- county_summary %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    join_name = toupper(str_remove_all(NAMELSADCO, \"(?i)\\\\s*COUNTY\"))\n  )\nmap1_data &lt;- pa_counties_projected %&gt;%\n  mutate(\n    join_name =COUNTY_NAM\n  ) %&gt;%\n  left_join(county_summary_clean, by = \"join_name\")\n\nggplot() +\n  geom_sf(data = map1_data, aes(fill = pct_vul_tracts_underserved), color = \"white\", size = 0.3) +\n  geom_sf(data = pa_hospitals_projected, color = \"#e41a1c\", size = 1, alpha = 1) +\n  scale_fill_viridis_c(\n    name = \"% Underserved\",\n    option = \"plasma\", \n    na.value = \"grey85\",\n    labels = scales::label_percent(scale = 1)\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges by County\",\n    subtitle = \"Percentage of vulnerable census tracts &gt; 15 miles from a hospital\",\n    caption = \"Data Source: 2022 ACS & PA Open Data\"\n  ) +\n  \n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 11, color = \"grey30\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\nunderserved_tracts&lt;-pa_vulnerable_projected%&gt;%\n  filter(is_underserved==TRUE)\n\nggplot()+\n  geom_sf(data=pa_counties_projected,fill=\"grey85\",color=\"grey95\",size=0.5)+\n    geom_sf(data = underserved_tracts,fill=\"#d95f02\", color = NA)+\n    geom_sf(data = pa_hospitals_projected,color=\"red\",size=1)+\nlabs(\n  title = \"Healthcare Deserts in Pennsylvania\",\n  subtitle = \"Vulnerable census tracts located &gt; 15 miles from the nearest hospital (highlighted in orange)\",\n caption = \"Data Source: 2022 ACS & PA Open Data\"\n)+\n  theme_void()+\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 11, color = \"grey30\"),\n    plot.margin = margin(10, 10, 10, 10)\n  )\n\n\n\n\n\n\n\n\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\nggplot(data = pa_vulnerable_projected, aes(x = dist_to_hospital_miles)) +\n  geom_histogram(binwidth = 2, fill = \"#4575b4\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = 15, color = \"#d73027\", linetype = \"dashed\", size = 1.2) +\n  annotate(\"text\", x = 15.5, y = Inf, label = \"15-mile Threshold\\n(Underserved)\", \n           vjust = 1.5, hjust = 0, color = \"#d73027\", fontface = \"bold\", size = 4) +\n  labs(\n    title = \"How Far Are Vulnerable Communities from Hospitals?\",\n    subtitle = \"Distribution of distances to the nearest hospital for vulnerable census tracts in PA\",\n    x = \"Distance to Nearest Hospital (Miles)\",\n    y = \"Number of Census Tracts\",\n    caption = \"Data Source: 2022 ACS & PA Open Data\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 11, color = \"grey30\"),\n    axis.title.x = element_text(margin = margin(t = 10), face = \"bold\"),\n    axis.title.y = element_text(margin = margin(r = 10), face = \"bold\"),\n    panel.grid.minor = element_blank() \n  )"
  },
  {
    "objectID": "Labs/Lab_2/lab2_template.html#part-3-bring-your-own-data-analysis",
    "href": "Labs/Lab_2/lab2_template.html#part-3-bring-your-own-data-analysis",
    "title": "Lab 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\n\nEnvironmental Justice\nGreen Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?”\n\nOperations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics\nPolicy relevance: Climate resilience, environmental justice, urban forestry investment\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/\n\n\nYour Analysis\nYour Task:\n\n# Load your additional dataset\nlibrary(sf)\nlibrary(tidyverse)\nphily_Recreation&lt;- st_read(\"C:/Users/yzhon/OneDrive - PennO365/Penn-Spring 2026/CPLN 592/cpln-5920-sp26-student-portfolio-template/Labs/Lab_2/PPR_Properties.geojson\")\n\nReading layer `PPR_Properties' from data source \n  `C:\\Users\\yzhon\\OneDrive - PennO365\\Penn-Spring 2026\\CPLN 592\\cpln-5920-sp26-student-portfolio-template\\Labs\\Lab_2\\PPR_Properties.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 504 features and 25 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.28353 ymin: 39.87117 xmax: -74.95865 ymax: 40.13186\nGeodetic CRS:  WGS 84\n\nprint(st_crs(phily_Recreation)$epsg)\n\n[1] 4326\n\nsummary(phily_Recreation)\n\n public_name        parent_name           nested          official_name     \n Length:504         Length:504         Length:504         Length:504        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    label              alias           dpp_asset_id       address_911       \n Length:504         Length:504         Length:504         Length:504        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   zip_code         address_brt        alias_address         acreage         \n Length:504         Length:504         Length:504         Min.   :1.925e-02  \n Class :character   Class :character   Class :character   1st Qu.:8.849e-01  \n Mode  :character   Mode  :character   Mode  :character   Median :3.563e+00  \n                                                          Mean   :2.007e+01  \n                                                          3rd Qu.:8.215e+00  \n                                                          Max.   :1.683e+03  \n property_classification   ppr_use          ppr_prog_district \n Length:504              Length:504         Length:504        \n Class :character        Class :character   Class :character  \n Mode  :character        Mode  :character   Mode  :character  \n                                                              \n                                                              \n                                                              \n ppr_ops_district   council_district   police_district    city_scale_maps   \n Length:504         Length:504         Length:504         Length:504        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n local_scale_maps   program_sites        comments            objectid    \n Length:504         Length:504         Length:504         Min.   :  1.0  \n Class :character   Class :character   Class :character   1st Qu.:126.8  \n Mode  :character   Mode  :character   Mode  :character   Median :252.5  \n                                                          Mean   :252.5  \n                                                          3rd Qu.:378.2  \n                                                          Max.   :504.0  \n  Shape__Area        Shape__Length               geometry  \n Min.   :1.328e+02   Min.   :   57.99   MULTIPOLYGON :504  \n 1st Qu.:6.102e+03   1st Qu.:  361.78   epsg:4326    :  0  \n Median :2.467e+04   Median :  715.52   +proj=long...:  0  \n Mean   :1.387e+05   Mean   : 1721.22                      \n 3rd Qu.:5.667e+04   3rd Qu.: 1228.31                      \n Max.   :1.164e+07   Max.   :91233.29                      \n\n\nQuestions to answer:\n\nWhat dataset did you choose and why?\nI used PPR_Properties.geojson,I selected this to analyze green space equity (Option C), as access to public parks is a crucial indicator of public health and urban environmental quality.\nWhat is the data source and date?\nThe data is sourced from OpenDataPhilly, not sure the exact date.\nHow many features does it contain?\n504\nWhat CRS is it in? Did you need to transform it?\n4326,yes, I change to 2272(PA south).\n\n\n\nPose a research question\n\nDo vulnerable census tracts in Philadelphia have equitable walking access to public parks and green spaces?\n\n\nConduct spatial analysis\n\nYour Task:\n\n# Your spatial analysis\nphily_Recreation_projected&lt;- st_transform(phily_Recreation,2272)\n\nphilly_tracts&lt;- pa_tracts_with_data %&gt;%\n  filter(str_detect(NAMELSADCO,\"Philadelphia County\"))%&gt;%\n  st_transform(2272)\n\nphilly_centroids&lt;- st_centroid(philly_tracts)\n\nparks_unioned &lt;- st_union(phily_Recreation_projected)\n\npark_to_tracts &lt;- st_distance(philly_centroids, parks_unioned)\n\nphilly_tracts$dist_to_park_miles &lt;- as.numeric(park_to_tracts) / 5280\n\nggplot() +\n  geom_sf(data = philly_tracts, aes(fill = dist_to_park_miles), color = NA) +\n  scale_fill_viridis_c(\n    name = \"Miles to Park\", \n    option = \"mako\", \n    direction = -1  \n  ) +\n  labs(\n    title = \"Park Accessibility in Philadelphia\",\n    subtitle = \"Distance from neighborhood centroids to the nearest recreation property\",\n    caption = \"Data: OpenDataPhilly & ACS\"\n  ) +\n  theme_void() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\nlibrary(knitr)\nlibrary(dplyr)\n\npark_access_summary &lt;- philly_tracts %&gt;%\n  st_drop_geometry() %&gt;% \n  mutate(\n    access_level = case_when(\n      dist_to_park_miles &lt;= 0.5 ~ \"0 - 0.5 miles (Excellent Walkability)\",\n      dist_to_park_miles &gt; 0.5 & dist_to_park_miles &lt;= 1.0 ~ \"0.5 - 1.0 miles (Good)\",\n      dist_to_park_miles &gt; 1.0 & dist_to_park_miles &lt;= 2.0 ~ \"1.0 - 2.0 miles (Fair)\",\n      dist_to_park_miles &gt; 2.0 ~ \"&gt; 2.0 miles (Park Desert)\",\n      TRUE ~ \"Unknown\"\n    )\n  ) %&gt;%\n  group_by(access_level) %&gt;%\n  summarize(\n    number_of_tracts = n()\n  ) %&gt;%\n  mutate(\n    percentage = paste0(round((number_of_tracts / sum(number_of_tracts)) * 100, 1), \"%\")\n  )\npark_access_summary %&gt;%\n  kable(\n    col.names = c(\"Distance to Nearest Park\", \"Number of Tracts\", \"Percentage (%)\"),\n    caption = \"Distribution of Park Accessibility across Philadelphia Census Tracts\",\n    align = \"lcc\"\n  )\n\n\nDistribution of Park Accessibility across Philadelphia Census Tracts\n\n\n\n\n\n\n\nDistance to Nearest Park\nNumber of Tracts\nPercentage (%)\n\n\n\n\n0 - 0.5 miles (Excellent Walkability)\n390\n95.6%\n\n\n0.5 - 1.0 miles (Good)\n17\n4.2%\n\n\n1.0 - 2.0 miles (Fair)\n1\n0.2%\n\n\n\n\n\nYour interpretation:\nBased on the spatial analysis, basic park accessibility in Philadelphia is exceptionally high. The summary statistics reveal that 95.6% of census tracts enjoy “Excellent Walkability”, being located within a 0.5-mile radius of a recreation property.However, the map highlights specific spatial gaps primarily in the extreme southern and far northeastern edges of the city. From a planning perspective, these are largely driven by distinct land-use patterns: the deep south is dominated by industrial zones and the Philadelphia International Airport (where residential population is minimal), while the Far Northeast features a more suburban, auto-oriented built environment where residents often have access to private yards, reducing the historical demand for dense public pocket parks. Overall, the city demonstrates strong spatial equity in baseline park access."
  },
  {
    "objectID": "Labs/Lab_2/lab2_template.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "Labs/Lab_2/lab2_template.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Lab 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nI clean up the template,and use echo:false to hide all the library set up code.I avoid using print() and head()this time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yiting Zhong-MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nI am first year MCP student.\n\n\n\n\nEmail: [yzhong06@upenn.edu]\nGitHub: [yitingzzzttt]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Yiting Zhong-MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "Yiting Zhong-MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Yiting Zhong-MUSA 5080 Portfolio",
    "section": "",
    "text": "I am first year MCP student."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Yiting Zhong-MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [yzhong06@upenn.edu]\nGitHub: [yitingzzzttt]"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html",
    "href": "Labs/Lab_1/lab1_template.html",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the CA Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/labs/lab_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Labs” menu."
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#scenario",
    "href": "Labs/Lab_1/lab1_template.html#scenario",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the CA Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#learning-objectives",
    "href": "Labs/Lab_1/lab1_template.html#learning-objectives",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#submission-instructions",
    "href": "Labs/Lab_1/lab1_template.html#submission-instructions",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/labs/lab_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Labs” menu."
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#data-retrieval",
    "href": "Labs/Lab_1/lab1_template.html#data-retrieval",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    med_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  state = my_state,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n\n# Hint: use mutate() with str_remove()\ncounty_data_clean &lt;- county_data %&gt;%\n  mutate(NAME = str_remove(NAME, \"County, California\"))\n\n# Display the first few rows\nhead(county_data_clean)\n\n# A tibble: 6 × 6\n  GEOID NAME         med_incomeE med_incomeM total_popE total_popM\n  &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 06001 \"Alameda \"        122488        1231    1663823         NA\n2 06003 \"Alpine \"         101125       17442       1515        206\n3 06005 \"Amador \"          74853        6048      40577         NA\n4 06007 \"Butte \"           66085        2261     213605         NA\n5 06009 \"Calaveras \"       77526        3875      45674         NA\n6 06011 \"Colusa \"          69619        5745      21811         NA"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#data-quality-assessment",
    "href": "Labs/Lab_1/lab1_template.html#data-quality-assessment",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nincome_analysis &lt;- county_data_clean %&gt;%\n  mutate(\n    moe_pct = (med_incomeM / med_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &lt;= 10 ~ \"Moderate Confidence\",\n      TRUE ~ \"Low Confidence\"\n    ),\n    unreliable_flag = if_else(moe_pct &gt; 10, \"Yes\", \"No\")\n  )\n\n# Create a summary showing count of counties in each reliability category\nreliability_summary &lt;- income_analysis %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = (n / sum(n)) * 100\n  )\n\nkable(reliability_summary, digits = 1, caption = \"Summary of Data Reliability by County\")\n\n\nSummary of Data Reliability by County\n\n\nreliability\nn\npercentage\n\n\n\n\nHigh Confidence\n42\n72.4\n\n\nLow Confidence\n5\n8.6\n\n\nModerate Confidence\n11\n19.0\n\n\n\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#high-uncertainty-counties",
    "href": "Labs/Lab_1/lab1_template.html#high-uncertainty-counties",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nhigh_uncertainty_top5 &lt;- income_analysis %&gt;%\n  arrange(desc(moe_pct))%&gt;%\n  slice(1:5)%&gt;%\n  select(\n    NAME,\n    med_incomeE,\n    med_incomeM,\n    moe_pct,\n    reliability\n  )\n\n# Format as table with kable() - include appropriate column names and caption\nkable(\nhigh_uncertainty_top5, \n      col.names = c(\"County Name\", \"Median Income (Estimate)\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\ndigits = 2,\ncaption = \"Top 5 counties with the highest Uncertainty\"\n)\n\n\nTop 5 counties with the highest Uncertainty\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income (Estimate)\nMargin of Error\nMOE %\nReliability\n\n\n\n\nMono\n82038\n15388\n18.76\nLow Confidence\n\n\nAlpine\n101125\n17442\n17.25\nLow Confidence\n\n\nSierra\n61108\n9237\n15.12\nLow Confidence\n\n\nTrinity\n47317\n5890\n12.45\nLow Confidence\n\n\nPlumas\n67885\n7772\n11.45\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]\n“The high MOE percentages in rural counties like Mono and Alpine (both over 17%) indicate that small sample sizes in sparsely populated areas lead to significant data instability. If algorithmic systems treat these”Low Confidence” estimates as absolute facts, they risk misallocating social services and reinforcing geographic inequities against rural communities. Consequently, these communities may be poorly served by automated decisions that fail to account for the inherent statistical uncertainty in their income data.”"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#focus-area-selection",
    "href": "Labs/Lab_1/lab1_template.html#focus-area-selection",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties&lt;- income_analysis%&gt;%\n  group_by(reliability) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(NAME, GEOID, med_incomeE, moe_pct, reliability)\n\n# Display the selected counties with their key characteristics\nkable(\nselected_counties, \n      col.names = c(\"County Name\", \"GEOID\", \"Median Income (Estimate)\", \"MOE %\", \"Reliability\"),\ndigits = 3,\ncaption = \"Representative Counties Selected for Each Reliability Level\"\n)\n\n\nRepresentative Counties Selected for Each Reliability Level\n\n\n\n\n\n\n\n\n\nCounty Name\nGEOID\nMedian Income (Estimate)\nMOE %\nReliability\n\n\n\n\nAlameda\n06001\n122488\n1.005\nHigh Confidence\n\n\nAlpine\n06003\n101125\n17.248\nLow Confidence\n\n\nAmador\n06005\n74853\n8.080\nModerate Confidence\n\n\n\n\n# Show: county name, median income, MOE percentage, reliability category\n\nComment on the output: [I selected “Alameda”, ““Amador”, and “Alpine” to represent High, Moderate, and Low Confidence levels respectively. ]"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#tract-level-demographics",
    "href": "Labs/Lab_1/lab1_template.html#tract-level-demographics",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nrace_var &lt;- c(\n  total_pop= \"B03002_001\",\n  white_alone=\"B03002_003\",\n  black=\"B03002_004\",\n  hispanic_latino=\"B03002_012\"\n)\n# Use get_acs() to retrieve tract-level data\ntarget_counties &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\ntract_data&lt;-get_acs(\n  geography = \"tract\",\n  variables = race_var,\n  state = my_state,\n  county = target_counties,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n  \n)\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\ntract_demographics &lt;- tract_data %&gt;%\n  mutate(\n    pct_white= (white_aloneE/total_popE)*100,\n    pct_black=(blackE/total_popE)*100,\n    pct_h_l=(hispanic_latinoE/total_popE)*100,\n    county_name = str_extract(NAME, \"(?&lt;=; ).*(?= County;)\"),\n    tract_name = str_extract(NAME, \"^[^,]*\"\n  ))\n# Create percentages for white, Black, and Hispanic populations\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#demographic-analysis",
    "href": "Labs/Lab_1/lab1_template.html#demographic-analysis",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhighest_h_l &lt;- tract_demographics %&gt;%\n  arrange(desc(pct_h_l)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME,pct_h_l, total_popE )\n# Calculate average demographics by county using group_by() and summarize()\naverage_demographics_couty &lt;- tract_demographics %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    tract_count= n(),\n    avg_white=mean(pct_white, na.rm = TRUE),\n    avg_black=mean(pct_black, na.rm = TRUE),\n    avg_h_l=mean( pct_h_l, na.rm = TRUE),\n  )\n# Show: number of tracts, average percentage for each racial/ethnic group\n\n# Create a nicely formatted table of your results using kable()\nkable(\naverage_demographics_couty, \n      col.names = c(\"County\", \"Total Tracts\", \"Avg White %\", \"Avg Black %\", \"Avg Hispanic %\"),\ndigits = 3,\ncaption = \"Demographic Analysis of Selected County\"\n)\n\n\nDemographic Analysis of Selected County\n\n\nCounty\nTotal Tracts\nAvg White %\nAvg Black %\nAvg Hispanic %\n\n\n\n\nAlameda\n379\n31.033\n10.706\n21.385\n\n\nAlpine\n1\n58.086\n0.000\n14.059\n\n\nAmador\n10\n75.665\n1.552\n14.873"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#moe-analysis-for-demographic-variables",
    "href": "Labs/Lab_1/lab1_template.html#moe-analysis-for-demographic-variables",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\nmoe_analysis &lt;- tract_data %&gt;%\n  mutate(\n  moe_white = ((white_aloneM/white_aloneE)*100),\n  moe_black = ((blackM/blackE)*100),\n  moe_h_l = ((hispanic_latinoM/hispanic_latinoE)*100),\n  moe_total_pop=((total_popM/total_popE)*100),\n  high_moe_flag = if_else(\n  (is.infinite(moe_white)& moe_white &gt;15)|\n  (is.infinite(moe_black)& moe_black &gt;15)|\n  (is.infinite(moe_h_l)& moe_h_l &gt;15),\n  \"High Uncertainty\",\"Reliable\"\n))\n\n# Create summary statistics showing how many tracts have data quality issues\nmoe_sum&lt;- moe_analysis %&gt;%\n  count(high_moe_flag) %&gt;%\n  mutate(pct =(n/sum(n))*100)\n\nkable(moe_sum, caption = \"Summary of Tract-Level Data Quality Issues\")\n\n\nSummary of Tract-Level Data Quality Issues\n\n\nhigh_moe_flag\nn\npct\n\n\n\n\nHigh Uncertainty\n14\n3.589744\n\n\nReliable\n376\n96.410256"
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#pattern-analysis",
    "href": "Labs/Lab_1/lab1_template.html#pattern-analysis",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n\npatten_analysis &lt;- moe_analysis %&gt;%\n  mutate(\n    pct_white=((white_aloneE/total_popE)*100),\n    pct_black=((blackE/total_popE)*100),\n    pct_h_l=((hispanic_latinoE/total_popE)*100)\n  )%&gt;%\n  group_by(high_moe_flag)%&gt;%\n  summarise(\n    avg_white=mean(pct_white, na.rm = TRUE),\n    avg_black=mean(pct_black, na.rm = TRUE),\n    avg_h_l=mean(pct_h_l, na.rm = TRUE),\n    avg_total=mean(total_popE, na.rm = TRUE),\n    tract_count=n()\n  )\n# Create a professional table showing the patterns\nkable(patten_analysis,\n      col.names = c(\"Data Quality Group\", \"Avg white %\", \"Avg Black %\", \"Avg Hispanic %\",\"Avg Total Pop\", \"Tract Counts\"),\ndigits = 2,\ncaption = \"Summary of Pattern Analysis\")\n\n\nSummary of Pattern Analysis\n\n\n\n\n\n\n\n\n\n\nData Quality Group\nAvg white %\nAvg Black %\nAvg Hispanic %\nAvg Total Pop\nTract Counts\n\n\n\n\nHigh Uncertainty\n48.83\n0.83\n12.96\n2729.64\n14\n\n\nReliable\n31.72\n10.75\n21.46\n4435.37\n376\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?] Tracts with sparse populations exhibit high uncertainty due to limited sample sizes, resulting in imprecise data and a significant margin of error. In analyses, data from white populations in high-uncertainty tracts appear unreliable not because of racial issues, but because residents in sparsely populated rural areas tend to be more ethnically homogeneous than those in diverse urban centers. Consequently, higher average proportions of white residents are observed within the “high uncertainty” group."
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#analysis-integration-and-professional-summary",
    "href": "Labs/Lab_1/lab1_template.html#analysis-integration-and-professional-summary",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here] 1. Across all four levels of analysis, a clear systematic pattern emerges: data reliability is inversely correlated with population density. At the county level, urban centers like Alameda exhibit high confidence with a margin of error (MOE) as low as 1.005%, whereas sparsely populated rural counties like Alpine suffer from low confidence with MOEs exceeding 17.2%. This trend persists at the tract level, where “High Uncertainty” areas have a significantly smaller average population (2,729) compared to “Reliable” tracts (4,435).\n\nThe communities facing the greatest risk of algorithmic bias are small, rural, and geographically isolated populations. Based on the pattern analysis, these “High Uncertainty” tracts often have a higher average percentage of White residents (48.83%) and significantly lower percentages of Black residents (0.83%) compared to urban centers. While urban minority communities benefit from the robust data of high-density areas, rural residents—regardless of race—are at risk of being “miscalculated” by algorithms that do not account for the high statistical noise inherent in small-sample data.\nThe primary driver of this bias risk is the inherent limitation of sampling in low-density areas. In a county like Alpine, which has a total population of only 1,515, the American Community Survey (ACS) sample size is so small that a few outlier responses can drastically skew the median income estimate. Algorithms treat these estimates as absolute facts, but in reality, they are “statistical flukes” caused by the geographic reality of rural life, creating a structural bias where data-rich urban areas receive more predictable and “fair” algorithmic treatment than data-sparse rural ones.\nTo mitigate these risks, the Department of Human Services should implement a “Reliability-Aware” Decision Framework. First, any community flagged with an MOE &gt; 10% should be diverted from fully automated processing to a manual administrative review. Second, the Department should supplement Census data with local administrative records (such as state tax or utility data) for rural tracts to triangulate income estimates."
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#specific-recommendations",
    "href": "Labs/Lab_1/lab1_template.html#specific-recommendations",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ndecision_framework &lt;- income_analysis %&gt;%\n  select( NAME, med_incomeE, moe_pct,total_popE,reliability,) %&gt;%\n  mutate(\n    algo_recommendation = case_when(\n      reliability == \"High Confidence\"~\"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\"~\"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\"~\"Requires manual review or additional data\"\n  ))\n# Format as a professional table with kable()\nkable(decision_framework,\n      col.names = c(\"county name\", \"median income\", \"MOE percentage\", \"Total population\",\"Reliability\",\"Policy Recommendation\"),\ndigits = 2,\ncaption = \"Decision Framework for Algorithm Implementation\")\n\n\nDecision Framework for Algorithm Implementation\n\n\n\n\n\n\n\n\n\n\ncounty name\nmedian income\nMOE percentage\nTotal population\nReliability\nPolicy Recommendation\n\n\n\n\nAlameda\n122488\n1.00\n1663823\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAlpine\n101125\n17.25\n1515\nLow Confidence\nRequires manual review or additional data\n\n\nAmador\n74853\n8.08\n40577\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nButte\n66085\n3.42\n213605\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCalaveras\n77526\n5.00\n45674\nHigh Confidence\nSafe for algorithmic decisions\n\n\nColusa\n69619\n8.25\n21811\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nContra Costa\n120020\n1.25\n1162648\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDel Norte\n61149\n7.16\n27462\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nEl Dorado\n99246\n3.36\n191713\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFresno\n67756\n1.43\n1008280\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGlenn\n64033\n6.19\n28657\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHumboldt\n57881\n3.68\n136132\nHigh Confidence\nSafe for algorithmic decisions\n\n\nImperial\n53847\n4.11\n179578\nHigh Confidence\nSafe for algorithmic decisions\n\n\nInyo\n63417\n8.60\n18829\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKern\n63883\n2.07\n906883\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKings\n68540\n3.29\n152515\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake\n56259\n4.34\n68024\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLassen\n59515\n5.97\n31873\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLos Angeles\n83411\n0.53\n9936690\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadera\n73543\n3.87\n157243\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMarin\n142019\n2.89\n260485\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMariposa\n60021\n8.82\n17130\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMendocino\n61335\n3.58\n91145\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMerced\n64772\n3.31\n282290\nHigh Confidence\nSafe for algorithmic decisions\n\n\nModoc\n54962\n9.80\n8651\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMono\n82038\n18.76\n13219\nLow Confidence\nRequires manual review or additional data\n\n\nMonterey\n91043\n2.09\n437609\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNapa\n105809\n2.82\n137384\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNevada\n79395\n4.82\n102322\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrange\n109361\n0.81\n3175227\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlacer\n109375\n1.70\n406608\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlumas\n67885\n11.45\n19650\nLow Confidence\nRequires manual review or additional data\n\n\nRiverside\n84505\n1.26\n2429487\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSacramento\n84010\n0.97\n1579211\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Benito\n104451\n5.23\n64753\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSan Bernardino\n77423\n1.04\n2180563\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Diego\n96974\n1.02\n3289701\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Francisco\n136689\n1.43\n851036\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Joaquin\n82837\n1.75\n779445\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Luis Obispo\n90158\n2.56\n281712\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Mateo\n149907\n1.75\n754250\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Barbara\n92332\n2.05\n445213\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Clara\n153792\n1.00\n1916831\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Cruz\n104409\n3.04\n268571\nHigh Confidence\nSafe for algorithmic decisions\n\n\nShasta\n68347\n3.63\n181852\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSierra\n61108\n15.12\n2916\nLow Confidence\nRequires manual review or additional data\n\n\nSiskiyou\n53898\n4.90\n44049\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSolano\n97037\n1.78\n450995\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSonoma\n99266\n2.00\n488436\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStanislaus\n74872\n1.83\n552063\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSutter\n72654\n4.71\n99101\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTehama\n59029\n6.95\n65484\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTrinity\n47317\n12.45\n15889\nLow Confidence\nRequires manual review or additional data\n\n\nTulare\n64474\n2.31\n473446\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTuolumne\n70432\n6.66\n54993\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nVentura\n102141\n1.50\n842009\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYolo\n85097\n2.74\n217141\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYuba\n66693\n4.19\n81705\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate] Alameda Butte Calaveras Contra Costa El Dorado Fresno Humboldt Imperial Kern Kings Lake Los Angeles Madera Marin Mendocino Merced Monterey Napa Nevada Orange Placer Riverside Sacramento San Bernardino San Diego San Francisco San Joaquin San Luis Obispo San Mateo Santa Barbara Santa Clara Santa Cruz Shasta Siskiyou Solano Sonoma Stanislaus Sutter Tulare Ventura Yolo Yuba These counties have large populations and exceptionally large sample sizes, resulting in an extremely low margin of error (MOE) of approximately 1%. Their income estimates are highly stable, and the risk of algorithmic bias in these areas is minimal, making them safe for automated funding allocation.\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed] Amador Colusa Del Norte Glenn Inyo Lassen Mariposa Modoc San Benito Tehama Tuolumne Although the data is generally accurate, it exhibits a certain degree of volatility. It is recommended that while implementing the algorithm, an output monitoring mechanism be established to periodically conduct random sampling and inspection of anomalous allocation cases, ensuring the algorithm does not develop bias due to data fluctuations.\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.] Alpine Mono Plumas Sierra Trinity These counties have sparse populations, resulting in extremely unstable data with errors exceeding 17%. Direct application of algorithms would lead to severe geographic inequities. A “manual verification” mechanism could be implemented for these counties, or supplementary investigations could be conducted using state government administrative records to compensate for the shortcomings of census sample data."
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#questions-for-further-investigation",
    "href": "Labs/Lab_1/lab1_template.html#questions-for-further-investigation",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.] 1.Has the reliability of data for these remote counties improved or deteriorated over the past decade as the Census Bureau updated its sampling methods? 2.Could incorporating “income” alongside other metrics help reduce bias in the final results? # Technical Notes\nData Sources: - U.S. Census Bureau, American Community Survey 2018-2022 5-Year Estimates - Retrieved via tidycensus R package on February 9, 2026.\nReproducibility: - All analysis conducted in R version 4.3.0 - Census API key required for replication - Complete code and documentation available at: [your portfolio URL]\nMethodology Notes: County and tract names were cleaned using regular expressions to remove repetitive legal suffixes (e.g., “; California”), ensuring the final visualizations and tables were policy-ready.\nLimitations: The focus was restricted to California (CA); findings regarding rural data uncertainty may vary in states with different administrative structures or population distributions."
  },
  {
    "objectID": "Labs/Lab_1/lab1_template.html#submission-checklist",
    "href": "Labs/Lab_1/lab1_template.html#submission-checklist",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/labs/lab_1/your_file_name.html"
  }
]